{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start import neccessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from finnomena_api import finnomenaAPI\n",
    "from finnomena_api.keys import keys\n",
    "import warnings\n",
    "import yfinance as yf\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "api = finnomenaAPI()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First, we define the funds from which we will extract the necessary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 10 string for ONE-UGG-RA\n",
      "we have 48 name-code to search\n",
      "we have 10 string for 0P00018JE1\n",
      "we have 48 name-code to search\n"
     ]
    }
   ],
   "source": [
    "#Create search list for Finnomena API\n",
    "#Read the CSV file, the CSV file will contain the funds code-name which we are tracking\n",
    "df_search_list = pd.read_csv(\"./csv/input.csv\")\n",
    "\n",
    "#The code below selects column 'code-name' and converts it into a \"list\"\n",
    "code_name_column= df_search_list['code-name'].tolist()\n",
    "\n",
    "#Then, we use the for loop to remove unnecessary string characters.\n",
    "search_list = []\n",
    "for i in range(len(code_name_column)):\n",
    "    list = code_name_column[i].strip()\n",
    "    search_list.append(list)\n",
    "#To ensure that string is correct for the subsequent search.\n",
    "print('we have {0} string for {1}'.format(len(search_list[0]),search_list[0])) \n",
    "#To confirming the number of code-name\n",
    "print('we have {0} name-code to search'.format(len(search_list))) \n",
    "\n",
    "\n",
    "#Now we create serach list for Yahoo API\n",
    "df_search_list = pd.read_csv(\"./csv/input.csv\")\n",
    "code_name_column= df_search_list['y-code'].tolist()\n",
    "\n",
    "search_list_fed = []\n",
    "for i in range(len(code_name_column)):\n",
    "    list = code_name_column[i].strip()\n",
    "    search_list_fed.append(list)\n",
    "print('we have {0} string for {1}'.format(len(search_list_fed[0]),search_list_fed[0])) \n",
    "print('we have {0} name-code to search'.format(len(search_list_fed)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Extracting\n",
    "#### Using Finnomena API to extract data \n",
    "\n",
    "In the end we will transform retrieved data into 3 columns\n",
    "* Finnomena Security Code (Fund Code)\n",
    "* NAV Date\n",
    "* NAV (Current Price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing  ONE-UGG-RA\n",
      "processing  KFGG-A\n",
      "processing  TMBGQG\n",
      "processing  K-WORLDX\n",
      "processing  K-USXNDQ-A(A)\n",
      "processing  K-US500X-A(A)\n",
      "processing  K-EUX\n",
      "processing  K-EUSMALL\n",
      "processing  K-JPX-A(A)\n",
      "processing  K-ASIAX\n",
      "processing  TMBAGLF\n",
      "processing  K-CHX\n",
      "processing  KT-ASHARES-A\n",
      "processing  K-CHINA-A(A)\n",
      "processing  P-CGREEN\n",
      "processing  K-VIETNAM\n",
      "processing  PRINCIPAL VNEQ-A\n",
      "processing  K-INDX\n",
      "processing  KFHHCARE-A\n",
      "processing  K-CHANGE-A(A)\n",
      "processing  KKP SEMICON-H\n",
      "processing  KT-FINANCE-A\n",
      "processing  UGIS-N\n",
      "processing  K-CASH\n",
      "processing  UOBID\n",
      "processing  UOBSD\n",
      "processing  K-GOLD-A(A)\n",
      "processing  K-OIL\n",
      "processing  KT-MINING\n",
      "processing  KS50LTF-C(L)\n",
      "processing  KDLTF-C(L)\n",
      "processing  K-USA-SSF\n",
      "processing  K-CHINA-SSF\n",
      "processing  K-CHANGE-SSF\n",
      "processing  KFLTFA50-D\n",
      "processing  KFLTFDIV\n",
      "processing  KFGGSSF\n",
      "processing  UGIS-SSF\n",
      "processing  BIG CAP-D LTF\n",
      "processing  VALUE-D LTF\n",
      "processing  KT-CHINABOND-A\n",
      "processing  KT-ENERGY\n",
      "processing  PCASH\n",
      "processing  KUSARMF\n",
      "processing  B-INNOTECHRMF\n",
      "processing  KFGGRMF\n",
      "processing  KT-ASHARES RMF\n",
      "processing  KGARMF\n",
      "finish processing\n",
      "missing funds is  []\n",
      "missing funds SUM 0\n",
      "total number of funds =  48\n",
      "current final reviewed nav_date:  2023-02-06\n"
     ]
    }
   ],
   "source": [
    "#Create a variable to store the unfindable search codename.\n",
    "missing = []\n",
    "df_funds_info = pd.DataFrame()\n",
    "\n",
    "#Utilize a for loop to traverse each search code-name, append it to a dataframe, and then save it as a csv file.\n",
    "with warnings.catch_warnings(): # suppress warning\n",
    "    warnings.simplefilter('ignore')\n",
    "    for i in range(len(search_list)):\n",
    "        try:\n",
    "            run_index = search_list[i]\n",
    "            dict_info = api.get_fund_info(run_index)\n",
    "            df_add = pd.DataFrame(data=[dict_info])\n",
    "            df_funds_info = df_funds_info.append(df_add,ignore_index=True)\n",
    "        except:\n",
    "            try:\n",
    "                #Storing the code-name that cannot be found, then setting all dictionary values to 0 and adding them to the dataframe as remarks.\n",
    "                print('cannot find: ', run_index)\n",
    "                missing.append(run_index)\n",
    "                dict_info =dict.fromkeys(dict_info, '')\n",
    "                dict_info['security_name'] = str(run_index)\n",
    "                df_add = pd.DataFrame(data=[dict_info])\n",
    "                # df_funds_info = df_funds_info.append(df_add,ignore_index=True)\n",
    "                df_funds_info = pd.concat([df_funds_info, df_add], ignore_index=True)\n",
    "            except:\n",
    "                print('Caught Error')\n",
    "        print('processing ', str(run_index))\n",
    "print('finish processing')\n",
    "print('missing funds is ', missing)\n",
    "print('missing funds SUM', len(missing))\n",
    "print('total number of funds = ',len(df_funds_info))\n",
    "print('current final reviewed nav_date: ', df_funds_info['nav_date'].max())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### At this point will still need one more columns which is \"fund_type\", we will use beautifulsoup for that purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting fund_types: ONE-UGG-RA\n",
      "extracting fund_types: KFGG-A\n",
      "extracting fund_types: TMBGQG\n",
      "extracting fund_types: K-WORLDX\n",
      "extracting fund_types: K-USXNDQ-A(A)\n",
      "extracting fund_types: K-US500X-A(A)\n",
      "extracting fund_types: K-EUX\n",
      "extracting fund_types: K-EUSMALL\n",
      "extracting fund_types: K-JPX-A(A)\n",
      "extracting fund_types: K-ASIAX\n",
      "extracting fund_types: TMBAGLF\n",
      "extracting fund_types: K-CHX\n",
      "extracting fund_types: KT-ASHARES-A\n",
      "extracting fund_types: K-CHINA-A(A)\n",
      "extracting fund_types: P-CGREEN\n",
      "extracting fund_types: K-VIETNAM\n",
      "extracting fund_types: PRINCIPAL VNEQ-A\n",
      "extracting fund_types: K-INDX\n",
      "extracting fund_types: KFHHCARE-A\n",
      "extracting fund_types: K-CHANGE-A(A)\n",
      "extracting fund_types: KKP SEMICON-H\n",
      "extracting fund_types: KT-FINANCE-A\n",
      "extracting fund_types: UGIS-N\n",
      "extracting fund_types: K-CASH\n",
      "extracting fund_types: UOBID\n",
      "extracting fund_types: UOBSD\n",
      "extracting fund_types: K-GOLD-A(A)\n",
      "extracting fund_types: K-OIL\n",
      "extracting fund_types: KT-MINING\n",
      "extracting fund_types: KS50LTF-C(L)\n",
      "extracting fund_types: KDLTF-C(L)\n",
      "extracting fund_types: K-USA-SSF\n",
      "extracting fund_types: K-CHINA-SSF\n",
      "extracting fund_types: K-CHANGE-SSF\n",
      "extracting fund_types: KFLTFA50-D\n",
      "extracting fund_types: KFLTFDIV\n",
      "extracting fund_types: KFGGSSF\n",
      "extracting fund_types: UGIS-SSF\n",
      "extracting fund_types: BIG CAP-D LTF\n",
      "extracting fund_types: VALUE-D LTF\n",
      "extracting fund_types: KT-CHINABOND-A\n",
      "extracting fund_types: KT-ENERGY\n",
      "extracting fund_types: PCASH\n",
      "extracting fund_types: KUSARMF\n",
      "extracting fund_types: B-INNOTECHRMF\n",
      "extracting fund_types: KFGGRMF\n",
      "extracting fund_types: KT-ASHARES RMF\n",
      "extracting fund_types: KGARMF\n",
      "finish extraction\n",
      "total number extract fund_types is:  48\n"
     ]
    }
   ],
   "source": [
    "#The code below will retrieve the fund type from the Finnomena website using the web scraping method.\n",
    "from bs4 import BeautifulSoup\n",
    "fund_types_list =[]\n",
    "for i in search_list:\n",
    "    try:\n",
    "        url = \"https://www.finnomena.com/fund/\" + str(i)\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        soup = soup.find_all('div', class_='detail-row')\n",
    "        fund_type_add = soup[2].find('div', class_='right').text\n",
    "        fund_types_list.append(fund_type_add)\n",
    "    except:\n",
    "        pass\n",
    "    print('extracting fund_types:', str(i))\n",
    "print('finish extraction')\n",
    "print('total number extract fund_types is: ', len(fund_types_list))\n",
    "\n",
    "fund_types_df = pd.DataFrame(fund_types_list)\n",
    "fund_types_df = fund_types_df.rename(columns = {0: 'fund_type'})\n",
    "\n",
    "#join fund_types to the df_funds_info dataframe\n",
    "df_funds_info = pd.concat([df_funds_info, fund_types_df], axis = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Extracting current price from Yahoo Site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y-code: 0P00018JE1 nav: 25.78 & nav_date: 2023-02-03\n",
      "y-code: 0P00018JE1 nav: 25.78 & nav_date: 2023-02-03\n",
      "y-code: 0P0000TJ9P nav: 36.51 & nav_date: 2023-02-03\n",
      "y-code: ACWI nav: 91.1 & nav_date: 2023-02-06\n",
      "y-code: QQQ nav: 303.59 & nav_date: 2023-02-06\n",
      "y-code: IVV nav: 411.8 & nav_date: 2023-02-06\n",
      "y-code: EXW1.DE nav: 42.35 & nav_date: 2023-02-06\n",
      "y-code: 0P0001DK0W.F nav: 16.29 & nav_date: 2023-02-06\n",
      "y-code: 1306.T nav: 2074.5 & nav_date: 2023-02-07\n",
      "y-code: AAXJ nav: 69.22 & nav_date: 2023-02-06\n",
      "y-code: 0P00000TKE nav: 49.32 & nav_date: 2023-02-06\n",
      "y-code: 82822.HK nav: 12.44 & nav_date: 2023-02-07\n",
      "y-code: 0P0001IM1D nav: 1336.46 & nav_date: 2023-02-06\n",
      "y-code: 0P000085UN nav: 123.2 & nav_date: 2023-02-03\n",
      "y-code: KGRN nav: 30.75 & nav_date: 2023-02-06\n",
      "Got error from yahoo api for ticker NONE, Error: {'code': 'Not Found', 'description': 'No data found, symbol may be delisted'}\n",
      "- NONE: No timezone found, symbol may be delisted\n",
      "THIS IS NOT FOUND y-code: none nav: none & nav_date: none\n",
      "Got error from yahoo api for ticker NONE, Error: {'code': 'Not Found', 'description': 'No data found, symbol may be delisted'}\n",
      "- NONE: No timezone found, symbol may be delisted\n",
      "THIS IS NOT FOUND y-code: none nav: none & nav_date: none\n",
      "y-code: INDY nav: 41.69 & nav_date: 2023-02-06\n",
      "y-code: 0P0000LZRE nav: 568.9 & nav_date: 2023-02-03\n",
      "y-code: 0P0001BDIT.L nav: 328.1 & nav_date: 2023-02-06\n",
      "y-code: SOXX nav: 419.17 & nav_date: 2023-02-06\n",
      "y-code: 0P00000SRT.F nav: 48.62 & nav_date: 2023-02-03\n",
      "y-code: 0P0000X9O0 nav: 16.34 & nav_date: 2023-02-03\n",
      "Got error from yahoo api for ticker NONE, Error: {'code': 'Not Found', 'description': 'No data found, symbol may be delisted'}\n",
      "- NONE: No timezone found, symbol may be delisted\n",
      "THIS IS NOT FOUND y-code: none nav: none & nav_date: none\n",
      "Got error from yahoo api for ticker NONE, Error: {'code': 'Not Found', 'description': 'No data found, symbol may be delisted'}\n",
      "- NONE: No timezone found, symbol may be delisted\n",
      "THIS IS NOT FOUND y-code: none nav: none & nav_date: none\n",
      "Got error from yahoo api for ticker NONE, Error: {'code': 'Not Found', 'description': 'No data found, symbol may be delisted'}\n",
      "- NONE: No timezone found, symbol may be delisted\n",
      "THIS IS NOT FOUND y-code: none nav: none & nav_date: none\n",
      "y-code: GLD nav: 173.82 & nav_date: 2023-02-06\n",
      "y-code: DBO nav: 14.44 & nav_date: 2023-02-06\n",
      "y-code: 0P0000SUYI.F nav: 76.33 & nav_date: 2023-02-06\n",
      "Got error from yahoo api for ticker NONE, Error: {'code': 'Not Found', 'description': 'No data found, symbol may be delisted'}\n",
      "- NONE: No timezone found, symbol may be delisted\n",
      "THIS IS NOT FOUND y-code: none nav: none & nav_date: none\n",
      "Got error from yahoo api for ticker NONE, Error: {'code': 'Not Found', 'description': 'No data found, symbol may be delisted'}\n",
      "- NONE: No timezone found, symbol may be delisted\n",
      "THIS IS NOT FOUND y-code: none nav: none & nav_date: none\n",
      "Got error from yahoo api for ticker TO ADD, Error: {'code': 'Not Found', 'description': 'No data found, symbol may be delisted'}\n",
      "- TO ADD: No timezone found, symbol may be delisted\n",
      "THIS IS NOT FOUND y-code: to add nav: none & nav_date: none\n",
      "Got error from yahoo api for ticker TO ADD, Error: {'code': 'Not Found', 'description': 'No data found, symbol may be delisted'}\n",
      "- TO ADD: No timezone found, symbol may be delisted\n",
      "THIS IS NOT FOUND y-code: to add nav: none & nav_date: none\n",
      "Got error from yahoo api for ticker TO ADD, Error: {'code': 'Not Found', 'description': 'No data found, symbol may be delisted'}\n",
      "- TO ADD: No timezone found, symbol may be delisted\n",
      "THIS IS NOT FOUND y-code: to add nav: none & nav_date: none\n",
      "Got error from yahoo api for ticker NONE, Error: {'code': 'Not Found', 'description': 'No data found, symbol may be delisted'}\n",
      "- NONE: No timezone found, symbol may be delisted\n",
      "THIS IS NOT FOUND y-code: none nav: none & nav_date: none\n",
      "Got error from yahoo api for ticker NONE, Error: {'code': 'Not Found', 'description': 'No data found, symbol may be delisted'}\n",
      "- NONE: No timezone found, symbol may be delisted\n",
      "THIS IS NOT FOUND y-code: none nav: none & nav_date: none\n",
      "Got error from yahoo api for ticker TO ADD, Error: {'code': 'Not Found', 'description': 'No data found, symbol may be delisted'}\n",
      "- TO ADD: No timezone found, symbol may be delisted\n",
      "THIS IS NOT FOUND y-code: to add nav: none & nav_date: none\n",
      "Got error from yahoo api for ticker TO ADD, Error: {'code': 'Not Found', 'description': 'No data found, symbol may be delisted'}\n",
      "- TO ADD: No timezone found, symbol may be delisted\n",
      "THIS IS NOT FOUND y-code: to add nav: none & nav_date: none\n",
      "Got error from yahoo api for ticker NONE, Error: {'code': 'Not Found', 'description': 'No data found, symbol may be delisted'}\n",
      "- NONE: No timezone found, symbol may be delisted\n",
      "THIS IS NOT FOUND y-code: none nav: none & nav_date: none\n",
      "Got error from yahoo api for ticker NONE, Error: {'code': 'Not Found', 'description': 'No data found, symbol may be delisted'}\n",
      "- NONE: No timezone found, symbol may be delisted\n",
      "THIS IS NOT FOUND y-code: none nav: none & nav_date: none\n",
      "Got error from yahoo api for ticker TO ADD, Error: {'code': 'Not Found', 'description': 'No data found, symbol may be delisted'}\n",
      "- TO ADD: No timezone found, symbol may be delisted\n",
      "THIS IS NOT FOUND y-code: to add nav: none & nav_date: none\n",
      "y-code: 0P00000JZX nav: 22.76 & nav_date: 2023-02-06\n",
      "Got error from yahoo api for ticker NONE, Error: {'code': 'Not Found', 'description': 'No data found, symbol may be delisted'}\n",
      "- NONE: No timezone found, symbol may be delisted\n",
      "THIS IS NOT FOUND y-code: none nav: none & nav_date: none\n",
      "y-code: 0P0000JM6Q nav: 101.27 & nav_date: 2023-02-03\n",
      "y-code: 0P00019SEJ nav: 29.03 & nav_date: 2023-02-03\n",
      "y-code: 0P00018JE1 nav: 25.78 & nav_date: 2023-02-03\n",
      "y-code: 0P0001IM1D nav: 1336.46 & nav_date: 2023-02-06\n",
      "y-code: 0P00000AWK nav: 67.65 & nav_date: 2023-02-06\n",
      "Finish extract nav and nav_date from yahoo site\n"
     ]
    }
   ],
   "source": [
    "#Utilize a for loop to traverse each search y_code, append it to a dataframe, and then save it as a csv file.\n",
    "y_fund_df = pd.DataFrame({'y_code':[],'y_nav_date':[],'y_nav':[],'y_link':[]})\n",
    "y_main_link = 'https://finance.yahoo.com/quote/'\n",
    "with warnings.catch_warnings(): # suppress warning\n",
    "    warnings.simplefilter('ignore')\n",
    "    for i in search_list_fed:\n",
    "        try:\n",
    "            #Retrieve data element using API\n",
    "            fund = yf.Ticker(str(i))\n",
    "            fund_his = fund.history(period=\"max\", interval=\"1d\")\n",
    "\n",
    "            #Transforming data by rounding to two decimal places, resetting the index, and then selecting only 'Date' and 'Close' columns.\n",
    "            #Initially, the index is date, but using.reset index, date becomes a column so that it can be selected.\n",
    "            fund_add = fund_his.round(2).sort_index(ascending=False)\n",
    "            fund_add.reset_index(inplace=True)\n",
    "            fund_add = fund_add[['Date','Close']]\n",
    "\n",
    "            #We formatted the 'Date' column as a date since we do not require time for fund management.\n",
    "            fund_add['Date'] = fund_add['Date'].dt.date\n",
    "            \n",
    "            #Create variables for y-code string, navigation, and navigation date.\n",
    "            y_code = str(i)\n",
    "            y_nav = fund_add['Close'].iloc[0]\n",
    "            y_nav_date = fund_add['Date'].iloc[0]\n",
    "            y_link = y_main_link + str(i)\n",
    "            print('y-code: {0} nav: {1} & nav_date: {2}'.format(str(i),y_nav, y_nav_date))\n",
    "\n",
    "            #Add y-code string, nav, and nav date in a dataframe\n",
    "            y_fund_df = y_fund_df.append({'y_code':y_code,'y_nav_date':y_nav_date,'y_nav':y_nav,'y_link':y_link}, ignore_index = True)\n",
    "        except:\n",
    "            #In the event that a search is not a fed fund, we will pass the value NaN to a dataframe.\n",
    "            try:\n",
    "                y_code = str(i)\n",
    "                y_nav = ('none')\n",
    "                y_nav_date = ('none')\n",
    "                y_link = ('none')\n",
    "                print('THIS IS NOT FOUND y-code: {0} nav: {1} & nav_date: {2}'.format(str(i),y_nav, y_nav_date))\n",
    "                y_fund_df = y_fund_df.append({'y_code':y_code,'y_nav_date':y_nav_date,'y_nav':y_nav,'y_link':y_link}, ignore_index = True)\n",
    "            except:\n",
    "                print('Error Caught')\n",
    "                break\n",
    "print('Finish extract nav and nav_date from yahoo site')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Combining Dataframe from Multiple Sourcese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we select the required columns and rename them for future use.\n",
    "with warnings.catch_warnings(): # suppress warning\n",
    "    warnings.simplefilter('ignore')\n",
    "    df_funds_tracking = df_funds_info[['security_name','nav_date','current_price','fund_type','feeder_fund']]\n",
    "    df_funds_tracking['mainlink'] = \"https://www.finnomena.com/fund/\"\n",
    "    df_funds_tracking['weblink'] = df_funds_tracking['mainlink']+ df_funds_tracking['security_name']\n",
    "    df_funds_tracking = df_funds_tracking[['security_name','nav_date','current_price','fund_type','feeder_fund','weblink']]\n",
    "    df_funds_tracking = df_funds_tracking.rename(columns = {'current_price':'nav'})\n",
    "    df_funds_tracking[['y_code','y_nav','y_nav_date','y_link']] = y_fund_df[['y_code','y_nav','y_nav_date','y_link']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export nav data as .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export file: df_funds_tracking.csv\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "#Export file to CSV\n",
    "print('export file: df_funds_tracking.csv')\n",
    "df_funds_tracking.to_csv('df_funds_tracking.csv',index=False)\n",
    "\n",
    "import time\n",
    "time.sleep(1)\n",
    "\n",
    "#Open the file\n",
    "import subprocess\n",
    "subprocess.Popen(['start', 'df_funds_tracking.csv'], shell=True)\n",
    "\n",
    "print('finish')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excel Automation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data type:  <class 'int'>\n",
      "data len:  48\n",
      "number of columns:  3\n",
      "Work book =  <Worksheet \"NAV Data\">\n",
      "ONE-UGG-RA\n",
      "2023-02-03\n",
      "23.286\n",
      "KFGG-A\n",
      "2023-02-03\n",
      "5.6645\n",
      "TMBGQG\n",
      "2023-02-03\n",
      "16.7168\n",
      "K-WORLDX\n",
      "2023-02-03\n",
      "11.2523\n",
      "K-USXNDQ-A(A)\n",
      "2023-02-03\n",
      "23.1137\n",
      "K-US500X-A(A)\n",
      "2023-02-03\n",
      "9.3916\n",
      "K-EUX\n",
      "2023-02-03\n",
      "16.0742\n",
      "K-EUSMALL\n",
      "2023-02-03\n",
      "12.9162\n",
      "K-JPX-A(A)\n",
      "2023-02-06\n",
      "15.2822\n",
      "K-ASIAX\n",
      "2023-02-03\n",
      "7.6138\n",
      "TMBAGLF\n",
      "2023-02-03\n",
      "13.7382\n",
      "K-CHX\n",
      "2023-02-06\n",
      "11.6128\n",
      "KT-Ashares-A\n",
      "2023-02-03\n",
      "8.6194\n",
      "K-CHINA-A(A)\n",
      "2023-02-03\n",
      "6.7171\n",
      "P-CGREEN\n",
      "2023-02-03\n",
      "8.2998\n",
      "K-VIETNAM\n",
      "2023-02-06\n",
      "11.6067\n",
      "PRINCIPAL VNEQ-A\n",
      "2023-02-06\n",
      "10.7344\n",
      "K-INDX\n",
      "2023-02-03\n",
      "13.9716\n",
      "KFHHCARE-A\n",
      "2023-02-03\n",
      "13.344\n",
      "K-CHANGE-A(A)\n",
      "2023-02-03\n",
      "19.0225\n",
      "KKP SEMICON-H\n",
      "2023-02-03\n",
      "9.2282\n",
      "KT-FINANCE-A\n",
      "2023-02-03\n",
      "24.4716\n",
      "UGIS-N\n",
      "2023-02-03\n",
      "10.8116\n",
      "K-CASH\n",
      "2023-02-06\n",
      "13.3108\n",
      "UOBID\n",
      "2023-02-06\n",
      "11.9204\n",
      "UOBSD\n",
      "2023-02-06\n",
      "12.9939\n",
      "K-GOLD-A(A)\n",
      "2023-02-06\n",
      "11.624\n",
      "K-OIL\n",
      "2023-02-03\n",
      "5.3338\n",
      "KT-MINING\n",
      "2023-02-03\n",
      "7.4596\n",
      "KS50LTF-C(L)\n",
      "2023-02-06\n",
      "12.1043\n",
      "KDLTF-C(L)\n",
      "2023-02-06\n",
      "16.901\n",
      "K-USA-SSF\n",
      "2023-02-03\n",
      "12.7476\n",
      "K-CHINA-SSF\n",
      "2023-02-03\n",
      "6.7423\n",
      "K-CHANGE-SSF\n",
      "2023-02-03\n",
      "19.0046\n",
      "KFLTFA50-D\n",
      "2023-02-06\n",
      "22.8152\n",
      "KFLTFDIV\n",
      "2023-02-06\n",
      "19.4421\n",
      "KFGGSSF\n",
      "2023-02-03\n",
      "5.9697\n",
      "UGIS-SSF\n",
      "2023-02-03\n",
      "10.8099\n",
      "BIG CAP-D LTF\n",
      "2023-02-06\n",
      "18.9826\n",
      "VALUE-D LTF\n",
      "2023-02-06\n",
      "24.5492\n",
      "KT-CHINABOND-A\n",
      "2023-02-03\n",
      "8.9938\n",
      "KT-ENERGY\n",
      "2023-02-03\n",
      "11.6759\n",
      "PCASH\n",
      "2023-02-06\n",
      "12.6041\n",
      "KUSARMF\n",
      "2023-02-03\n",
      "5.4256\n",
      "B-INNOTECHRMF\n",
      "2023-02-02\n",
      "19.3844\n",
      "KFGGRMF\n",
      "2023-02-03\n",
      "5.9429\n"
     ]
    }
   ],
   "source": [
    "from openpyxl import Workbook, load_workbook\n",
    "from openpyxl.utils import get_column_letter\n",
    "import pandas as pd\n",
    "\n",
    "to_add_df =pd.read_csv('df_funds_tracking.csv')\n",
    "to_add_df = to_add_df[['security_name','nav_date','nav']]\n",
    "\n",
    "file_name = '../port-management.xlsx'\n",
    "wb = load_workbook(file_name)\n",
    "row_num = pd.read_csv('df_funds_tracking.csv')\n",
    "\n",
    "row_num = len(row_num)\n",
    "print('data type: ',type(row_num))\n",
    "print('data len: ', row_num)\n",
    "\n",
    "number_of_columns = to_add_df.shape[1]\n",
    "print('number of columns: ', number_of_columns)\n",
    "\n",
    "ws = wb['NAV Data']\n",
    "print('Work book = ', ws)\n",
    "\n",
    "\n",
    "# This is fixed code to manipulate a certain excel fiel\n",
    "# 'col' is the starting columns\n",
    "# 'last_col' is the last column\n",
    "#****** I USE WHILE LOOP HERE BECAUSE USING get_column_letter within the for loop will cause error due to the library itself\n",
    "col_excel_start = 2 #starting col >>> [1] =  column A [2] = column B [2] (in excel file it us column a as index)\n",
    "col_offset = col_excel_start #for py to offset in range wqhen use iloc\n",
    "last_col = number_of_columns+2\n",
    "xl_col = []\n",
    "while col_excel_start <= last_col:\n",
    "    add = get_column_letter(col_excel_start)\n",
    "    xl_col.append(add)\n",
    "    col_excel_start+=1\n",
    "\n",
    "col= 2 #re assign starting col  to >>> 2 = B\n",
    "xl_col #this store the list of columns and excel we need (as string)\n",
    "\n",
    "\n",
    "\n",
    "#To skip the header we set start_row to 2\n",
    "start_row = 2\n",
    "row_offset = start_row #for py as we don't want title\n",
    "for row in range(start_row,row_num):\n",
    "    for colxl,colpy in zip(xl_col,range(col,last_col)):\n",
    "        if not pd.isna(to_add_df.iloc[row - row_offset, 2]):\n",
    "            ws[colxl + str(row)].value = to_add_df.iloc[row-row_offset,colpy-col_offset]\n",
    "            print(to_add_df.iloc[row-row_offset,colpy-col_offset])\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "\n",
    "wb.save(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start retrieving process of historical data to plot candlestick and timeseries chart"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to Retrieve historical price from Yahoo Finance (Single Security)\n",
    "<details><summary><font color=\"yellow\">Click here for the solution</font></summary>\n",
    "\n",
    "```python\n",
    "##### Valid periods: 1d,5d,1mo,3mo,6mo,1y,2y,5y,10y,ytd,max Either Use period parameter or use start and end\n",
    "name_y = '0P00018JE1'\n",
    "fund_y = yf.Ticker(name_y)\n",
    "his_y = fund_y.history(period=\"max\", interval=\"1d\")\n",
    "his_y = his_y.round(2).sort_index(ascending=False)\n",
    "his_y.reset_index(inplace=True)\n",
    "his_y['Date'] = his_y['Date'].dt.date\n",
    "his_y.to_excel('his_y_'+str(name_y)+'.xlsx')\n",
    "##### print(his_y)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to retrieve historical price from Finnomena (Single Security)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><font color=\"yellow\">Click here for the solution</font></summary>\n",
    "\n",
    "```python\n",
    "name_f = 'TMBGQG'\n",
    "fund_f = api.get_fund_price(name_f)\n",
    "his_f = fund_f.round(2).sort_index(ascending=False)\n",
    "his_f.set_index('date', inplace=True)\n",
    "print(his_f)\n",
    "##### his_f.reset_index(inplace=True)\n",
    "##### his_f = his_f[['date','price']]\n",
    "his_f.to_csv('his_f_'+str(name_f)+'.csv')\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resample from Day interval to Weekly interval for a single column dataframe (Single Security)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><font color=\"yellow\">Click here for the solution</font></summary>\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "###### load data into a DataFrame\n",
    "df = pd.read_csv('his_f_ONE-UGG-RA - Copy.csv', parse_dates=['date'])\n",
    "###### set 'date' column as the DataFrame's index\n",
    "df.set_index('date', inplace=True)\n",
    "###### group data by week and take the mean of each group\n",
    "df_weekly = df.resample('W-TUE').max()\n",
    "df_weekly = df_weekly.round(2).sort_index(ascending=False)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to resameple data from Daily interval into Weekly interval: YAHOO FINANCE (Single Security)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><font color=\"yellow\">Click here for the solution</font></summary>\n",
    "\n",
    "```python\n",
    "# Valid periods: 1d,5d,1mo,3mo,6mo,1y,2y,5y,10y,ytd,max Either Use period parameter or use start and end\n",
    "name_y = '0P00018JE1'\n",
    "fund_y = yf.Ticker(name_y)\n",
    "df = fund_y.history(period=\"max\", interval=\"1d\")\n",
    "df = df.round(2).sort_index(ascending=False)\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# Set the date column as the index\n",
    "df.set_index('Date', inplace=True)\n",
    "df.index = pd.to_datetime(df.index)\n",
    "\n",
    "# Resample the data to weekly frequency, using the 'Close' column as the value\n",
    "df_weekly = df.resample('W-MON').agg({'Open': 'first', 'High': 'max', 'Low': 'min', 'Close': 'last'})\n",
    "df_weekly = df_weekly.rename(columns = {'Close':'Close Feeder'})\n",
    "df_weekly[['Close','Code', 'Security Filter']]=['',name_y,name_y]\n",
    "df_weekly['Date'] = df_weekly.index\n",
    "df_weekly = df_weekly[['Open','High','Low','Close','Close Feeder','Code','Security Filter','Date']]\n",
    "\n",
    "# Print the weekly data\n",
    "df_weekly = df_weekly.round(2).sort_index(ascending=False)\n",
    "# df_weekly.to_csv('df_weekly'+str(name))\n",
    "print(df_weekly)\n",
    "```\n",
    "\n",
    "\n",
    "</details>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resample from daily interval into weekly interval: Finnomena (Single Security)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><font color=\"yellow\">Click here for the solution</font></summary>\n",
    "\n",
    "\n",
    "````python\n",
    "name = 'ONE-UGG-RA'\n",
    "fund_f = api.get_fund_price(name)\n",
    "df = fund_f.round(2).sort_index(ascending=False)\n",
    "df.set_index('date', inplace=True)\n",
    "df.to_csv('his_f_'+str(name)+'.csv')\n",
    "df.index = pd.to_datetime(df.index)\n",
    "\n",
    "\n",
    "# Resample the data into weekly intervals\n",
    "df_weekly = df.resample(\"W-MON\").agg({\"price\": [\"first\", \"max\", \"min\", \"last\"]})\n",
    "\n",
    "# Rename the columns\n",
    "df_weekly.columns = [\"Open\", \"High\", \"Low\", \"Close\"]\n",
    "df_weekly = df_weekly.round(2).sort_index(ascending=False)\n",
    "# df_weekly.to_csv('df_weekly'+str(name)+'.csv')\n",
    "df_weekly[['Close Feeder','Code', 'Security Filter']]=['',name,name]\n",
    "print(df_weekly)\n",
    "````\n",
    "\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Search list to filter only necessary funds which has fed a fund"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 10 string for ONE-UGG-RA\n",
      "we have 30 name-code to search\n",
      "we have 10 string for 0P00018JE1\n",
      "we have 30 name-code to search\n"
     ]
    }
   ],
   "source": [
    "#The procedure was the same as when we created a search list for NAV data, but this time we restricted the search list to global funds.\n",
    "\n",
    "df_search_list = pd.read_csv(\"./csv/input.csv\")\n",
    "\n",
    "to_drop = []\n",
    "for i in range(len(df_search_list)):\n",
    "    if df_search_list.iloc[i,1] == 'none' or df_search_list.iloc[i,1] == 'to add':\n",
    "        to_drop.append(i)\n",
    "df_search_list = df_search_list.drop(df_search_list.index[to_drop])\n",
    "search_list = df_search_list.iloc[:,0].tolist()\n",
    "search_list_fed = df_search_list.iloc[:,1].tolist()\n",
    "\n",
    "for i in range(len(search_list)):\n",
    "    search_list[i] = search_list[i].strip()\n",
    "print('we have {0} string for {1}'.format(len(search_list[0]),search_list[0])) \n",
    "print('we have {0} name-code to search'.format(len(search_list)))\n",
    "\n",
    "for i in range(len(search_list_fed)):\n",
    "    search_list_fed[i] = search_list_fed[i].strip()\n",
    "print('we have {0} string for {1}'.format(len(search_list_fed[0]),search_list_fed[0])) \n",
    "print('we have {0} name-code to search'.format(len(search_list_fed)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resample from daily interval into weekly interval: Finnomena\n",
    "\n",
    "The data in dataframe contain only close price so we perform some transformation.\n",
    "1. Create a new column titled 'Open' that will contain the oldest price value within a weekly interval.\n",
    "2. Create a new column titled \"High\" that will contain the highest price value within a weekly interval.\n",
    "3. Create a new column titled \"Low\" that will contain the lowest price value from the \"price\" column within the weekly interval.\n",
    "4. Create a new column titled \"Close\" that contains the most recent value from the \"price\" column within a weekly interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed:0 ONE-UGG-RA \n",
      "processed:1 KFGG-A \n",
      "processed:2 TMBGQG \n",
      "processed:3 K-WORLDX \n",
      "processed:4 K-USXNDQ-A(A) \n",
      "processed:5 K-US500X-A(A) \n",
      "processed:6 K-EUX \n",
      "processed:7 K-EUSMALL \n",
      "processed:8 K-JPX-A(A) \n",
      "processed:9 K-ASIAX \n",
      "processed:10 TMBAGLF \n",
      "processed:11 K-CHX \n",
      "processed:12 KT-ASHARES-A \n",
      "processed:13 K-CHINA-A(A) \n",
      "processed:14 P-CGREEN \n",
      "processed:15 K-INDX \n",
      "processed:16 KFHHCARE-A \n",
      "processed:17 K-CHANGE-A(A) \n",
      "processed:18 KKP SEMICON-H \n",
      "processed:19 KT-FINANCE-A \n",
      "processed:20 UGIS-N \n",
      "processed:21 K-GOLD-A(A) \n",
      "processed:22 K-OIL \n",
      "processed:23 KT-MINING \n",
      "processed:24 KT-ENERGY \n",
      "processed:25 KUSARMF \n",
      "processed:26 B-INNOTECHRMF \n",
      "processed:27 KFGGRMF \n",
      "processed:28 KT-ASHARES RMF \n",
      "processed:29 KGARMF \n",
      "missing funds is  []\n",
      "missing funds SUM 0\n",
      "total number of funds =  8605\n"
     ]
    }
   ],
   "source": [
    "#Utilize a for loop to append each historical price the Resample the data from daily interval into weekly interval\n",
    "\n",
    "df_funds_price = pd.DataFrame()\n",
    "missing =[]\n",
    "with warnings.catch_warnings(): # suppress warning\n",
    "    warnings.simplefilter('ignore')\n",
    "    for i in range(len(search_list)):\n",
    "        #FOR HISTORYCAL DATA INFORMATION\n",
    "        try:\n",
    "            run_index = search_list[i]\n",
    "            fund_f = api.get_fund_price(run_index)\n",
    "            df = fund_f.round(2).sort_index(ascending=False)\n",
    "\n",
    "            #Set date as an index\n",
    "            df.set_index(\"date\", inplace=True)\n",
    "\n",
    "            #Reformat the index as datetime type\n",
    "            df.index = pd.to_datetime(df.index)\n",
    "\n",
    "            # Resample the data into weekly intervals\n",
    "            df_weekly = df.resample(\"W-MON\").agg({\"price\": [\"first\", \"max\", \"min\", \"last\"]})\n",
    "\n",
    "            # Rename the columns 'first', 'max', 'min', and 'last'\n",
    "            df_weekly.columns = [\"Open\", \"High\", \"Low\", \"Close Feeder\"]\n",
    "            \n",
    "            df_weekly = df_weekly.sort_index(ascending=False)\n",
    "            # df_weekly.to_csv('df_weekly'+str(name)+'.csv')\n",
    "            df_weekly[['Close Fed','Code', 'Security Filter']]=['NaN',run_index,run_index]\n",
    "            df_weekly['Date']=df_weekly.index\n",
    "            df_weekly['Date'] = df_weekly['Date'].dt.date\n",
    "            df_weekly['Year'] = pd.to_datetime(df_weekly['Date']).dt.year\n",
    "            df_weekly['Categories'] = 'Local'\n",
    "            df_funds_price = pd.concat([df_funds_price, df_weekly], ignore_index=True)\n",
    "            print('processed:{0} {1} '.format(i,run_index))\n",
    "        except:\n",
    "            try:\n",
    "                print('Cannot find: ', run_index)\n",
    "                missing.append(run_index)\n",
    "                df_weekly =dict.fromkeys(df_weekly, 'Error')\n",
    "                df_weekly['code'] = str(run_index)\n",
    "                df_weekly = pd.DataFrame(data=[df_weekly])\n",
    "                # df_funds_info = df_funds_info.append(df_add,ignore_index=True)\n",
    "                df_funds_price = pd.concat([df_funds_price, df_weekly], ignore_index=True)\n",
    "            except:\n",
    "                print('Caught Error')\n",
    "print('missing funds is ', missing)\n",
    "print('missing funds SUM', len(missing))\n",
    "print('total number of funds = ',len(df_funds_price))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resameple data from Daily interval into Weekly interval: YAHOO FINANCE\n",
    "\n",
    "1. The 'Open' column will contain the price from the 'Close' column on the earliest date within the weekly interval.\n",
    "2. The 'High' column will contain the highest price from 'Close' column with in the weekly interval\n",
    "3. The 'Low' column will contain the lowest price from 'Close' column within the weekly interval\n",
    "4. The 'Close' column will contain the latest price within its own column within the weekly interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed:0 0P00018JE1 \n",
      "processed:1 0P00018JE1 \n",
      "processed:2 0P0000TJ9P \n",
      "processed:3 ACWI \n",
      "processed:4 QQQ \n",
      "processed:5 IVV \n",
      "processed:6 EXW1.DE \n",
      "processed:7 0P0001DK0W.F \n",
      "processed:8 1306.T \n",
      "processed:9 AAXJ \n",
      "processed:10 0P00000TKE \n",
      "processed:11 82822.HK \n",
      "processed:12 0P0001IM1D \n",
      "processed:13 0P000085UN \n",
      "processed:14 KGRN \n",
      "processed:15 INDY \n",
      "processed:16 0P0000LZRE \n",
      "processed:17 0P0001BDIT.L \n",
      "processed:18 SOXX \n",
      "processed:19 0P00000SRT.F \n",
      "processed:20 0P0000X9O0 \n",
      "processed:21 GLD \n",
      "processed:22 DBO \n",
      "processed:23 0P0000SUYI.F \n",
      "processed:24 0P00000JZX \n",
      "processed:25 0P0000JM6Q \n",
      "processed:26 0P00019SEJ \n",
      "processed:27 0P00018JE1 \n",
      "processed:28 0P0001IM1D \n",
      "processed:29 0P00000AWK \n",
      "missing funds is  []\n",
      "missing funds SUM 0\n",
      "total number of funds =  11719\n"
     ]
    }
   ],
   "source": [
    "#Utilize a for loop to append each historical price the Resample the data from daily interval into weekly interval\n",
    "\n",
    "df_funds_price_f = pd.DataFrame()\n",
    "missing =[]\n",
    "with warnings.catch_warnings(): # suppress warning\n",
    "    warnings.simplefilter('ignore')\n",
    "    for i in range(len(search_list_fed)):\n",
    "        #FOR HISTORYCAL DATA INFORMATION\n",
    "        try:\n",
    "            # Valid periods: 1d,5d,1mo,3mo,6mo,1y,2y,5y,10y,ytd,max Either Use period parameter or use start and end\n",
    "            run_index = search_list_fed[i]\n",
    "            run_index_filter = search_list[i]\n",
    "            fund_y = yf.Ticker(run_index)\n",
    "            df = fund_y.history(period=\"max\", interval=\"1d\")\n",
    "            if len(df) == 0:\n",
    "                print('Cannot find: ', run_index)\n",
    "                pass\n",
    "            else:\n",
    "                df = df.round(2).sort_index(ascending=False)\n",
    "                df.reset_index(inplace=True)\n",
    "                # Set the date column as the index\n",
    "                df.set_index('Date', inplace=True)\n",
    "                df.index = pd.to_datetime(df.index)\n",
    "                # Resample the data to weekly frequency, using the 'Close' column as the value\n",
    "                df_weekly = df.resample('W-MON').agg({'Open': 'first', 'High': 'max', 'Low': 'min', 'Close': 'last'})\n",
    "                df_weekly = df_weekly.rename(columns = {'Close':'Close Fed'})\n",
    "                df_weekly[['Close Feeder','Code', 'Security Filter','Categories']]=['NaN',run_index,run_index_filter,'Global']\n",
    "                df_weekly['Date'] = df_weekly.index\n",
    "                df_weekly['Date'] = df_weekly['Date'].dt.date\n",
    "                df_weekly['Year'] = pd.to_datetime(df_weekly['Date']).dt.year\n",
    "                df_weekly = df_weekly[['Open','High','Low','Close Feeder','Close Fed','Code','Security Filter','Date','Categories','Year']]\n",
    "                # Print the weekly data\n",
    "                df_weekly = df_weekly.round(2).sort_index(ascending=False)\n",
    "                # df_weekly.to_csv('df_weekly'+str(name))\n",
    "                df_funds_price_f = pd.concat([df_funds_price_f, df_weekly], ignore_index=True)\n",
    "                print('processed:{0} {1} '.format(i,run_index))\n",
    "        except:\n",
    "            print('Caught Error')\n",
    "            break\n",
    "print('missing funds is ', missing)\n",
    "print('missing funds SUM', len(missing))\n",
    "print('total number of funds = ',len(df_funds_price_f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine historical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_funds_price_union = pd.concat([df_funds_price, df_funds_price_f], ignore_index=True)\n",
    "df_funds_price_union.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Data Before put to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open               0\n",
      "High               0\n",
      "Low                0\n",
      "Close Feeder       0\n",
      "Close Fed          0\n",
      "Code               0\n",
      "Security Filter    0\n",
      "Date               0\n",
      "Year               0\n",
      "Categories         0\n",
      "dtype: int64\n",
      "Open               float64\n",
      "High               float64\n",
      "Low                float64\n",
      "Close Feeder        object\n",
      "Close Fed           object\n",
      "Code                object\n",
      "Security Filter     object\n",
      "Date                object\n",
      "Year                 int64\n",
      "Categories          object\n",
      "dtype: object\n",
      "Open               float64\n",
      "High               float64\n",
      "Low                float64\n",
      "Close Feeder       float64\n",
      "Close Fed          float64\n",
      "Code                object\n",
      "Security Filter     object\n",
      "Date                object\n",
      "Year                 int64\n",
      "Categories          object\n",
      "dtype: object\n",
      "20309\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Open', 'High', 'Low', 'Close Feeder', 'Close Fed', 'Code',\n",
       "       'Security Filter', 'Date', 'Year', 'Categories'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_funds_price_union.isna().sum())\n",
    "print(df_funds_price_union.dtypes)\n",
    "df_funds_price_union['Open'] = df_funds_price_union['Open'].astype(float)\n",
    "df_funds_price_union['High'] = df_funds_price_union['High'].astype(float)\n",
    "df_funds_price_union['Low'] = df_funds_price_union['Low'].astype(float)\n",
    "df_funds_price_union['Close Feeder'] = df_funds_price_union['Close Feeder'].astype(float)\n",
    "df_funds_price_union['Close Fed'] = df_funds_price_union['Close Fed'].astype(float)\n",
    "print(df_funds_price_union.dtypes)\n",
    "print(len(df_funds_price_union))\n",
    "df_funds_price_union.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you want to export as csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````python\n",
    "df_funds_price_union.to_csv('df_funds_price_union.csv')\n",
    "subprocess.Popen(['start', 'df_funds_price_union.csv'], shell=True)\n",
    "````"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the historical data to google sheet which will linked to Tableau dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pygsheets\n",
    "GSHEET_NAME = 'makeone'\n",
    "\n",
    "df = df_funds_price_union\n",
    "\n",
    "creds = 'pkong-credential.json'\n",
    "api = pygsheets.authorize(service_file=creds)\n",
    "wb = api.open(GSHEET_NAME)\n",
    "# open the sheet by name\n",
    "sheet = wb.worksheet_by_title(f'Sheet1')\n",
    "sheet.clear(start='A1', end=None, fields='*')\n",
    "sheet.set_dataframe(df, (1,1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Link Google Sheet](https://docs.google.com/spreadsheets/d/1kpQeTokjN-G-gPOpmZbaLK0LNpuDZUzqTYNEFiu-6OU/edit#gid=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"800\"\n",
       "            src=\"https://public.tableau.com/app/profile/pongpisut.kongdan/viz/FundsDashboard_16752377846780/Dashboard1\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x29d3f2c4c40>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "url = 'https://public.tableau.com/app/profile/pongpisut.kongdan/viz/FundsDashboard_16752377846780/Dashboard1'\n",
    "IFrame(src=url, width=800, height=800)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Link Tableau]('https://public.tableau.com/app/profile/pongpisut.kongdan/viz/FundsDashboard_16752377846780/Dashboard1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d6338a2c909abaef21d4393d5b8ceae84b7b22a551ec24aec08ea09a6ac50e0c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
